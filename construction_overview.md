
# **MESH Dataset - Construction Overview**
This section provides insight into how the datasets were constructed.

## **Setting Dataset**
Here are the steps involved in generating questions about the presence of different objects in video clips:
1. **Object Extraction**: Objects in the video clips are extracted using TVQA+ bounding box dataset with Deepseek-V3 and manually designed prompt.
2. **Scene Location Extraction**: The location of the scene in the video clips is extracted using ```Qwen2VL-72B-Int4``` and verified by human annotators.
3. **Similar Object Generation**: By utilizing the extracted objects and scene locations, objects with similar shapes to the original ones that do not exist in the given locations are generated by prompting Deepseek-V3, acting as non-existing objects.
4. **Question Formulation**: Both existing and non-existing objects are employed to formulate various types of questions related to objects.


## **Character Dataset**
Here are the steps involved in generating questions about the presence of different individuals in video clips:
1. **Feature Extraction**: GPT-4o-mini is employed to extract up to eight types of features of a person in a frame of a video clip. These features include:
    - Garment type: e.g., t-shirt, jacket, coat, button-up shirt, blouse, etc.
    - Garment color: e.g., red, blue, yellow, green, etc.
    - Gender: Male or female.
    - Glasses: Whether the person wears glasses or not.
    - Garment sleeve: e.g., long sleeves, short sleeves, no sleeves, and half sleeves.
    - Garment collar: Whether the upper garment has a collar or not.
    - Garment pocket: Whether the upper garment has pockets or not.
    - Garment Shade: Whether the overall color of the upper garment is dark or light.
2. **Human Verification**: The extracted features are verified and corrected by human annotators.
3. **Feature Replacement**: The original features of a person in the video clip are substituted with dissimilar features to create a non-existing person.
4. **Question Formulation**: By utilizing both existing and non-existing persons, various types of questions related to individuals are generated using the Deepseek tool.



## **Stage (Non-Dialogue) Dataset**
Here are the steps involved in generating questions about the presence of different actions in video clips:

1. **Conversation Content Removal**: Remove any questions that related to the conversation content in the TVQA+ questions dataset using Deepseek-V2 with manually designed prompts.
2. **Action Extraction**: Actions from the TVQA+ questions dataset are extracted, using the Deepseek. The extracted motion follows the format "A does B," where A and B represent individuals or objects.
3. **Name Replacement**: Names in the motion are substituted with the characteristics of the respective individuals, such as their clothing and gender.
4. **Hallucination Types**: By combining the person "A" and the action "does B" from different sources, four different hallucination types are proposed:
    - **Action out of Video (AOV)**: Questions formulated by combining existing individuals from the video with action drawn from other videos.
    - **Character out of Video (COV)**: Questions formulated by individuals not present in the video and action existing within it.
    - **Similar Action (SA)**: Questions formulated by modifying existing video events. The altered event resembles the original but is not present in the video; for instance, transforming "open the door" into "closed the window.
    - **Mixed in Video (MIV)**: Questions formulated by existing individuals and action from the video, but the individuals involved in each action are interchanged. For example, merging descriptions like "A woman wearing a blue sweater is cooking" and "A man wearing a gray t-shirt is sitting on the couch" to create scenarios where "A man wearing a gray t-shirt is cooking" and "A woman wearing a blue sweater is sitting on the couch."
5. **Question Generation**: Using both existing and non-existing actions, questions related to action are formulated.

## **Stage (Dialogue) Dataset**
Here are the steps involved in generating questions about the presence of different "talking" behavior of different characters in video clips:

1. **Talking Period Extraction**: The time period of which character is talking is extracted from the TVQA+ subtitle dataset. 
2. **Hallucination Types**: By combining "talking" behavior of different characters from different time period, two different hallucination types are proposed:
    - **Character out of Video (COV)**: Questions formulated by combining character not present in the video and one random talking period in the video.
    - **Character in Video (CIV)**: Questions formulated by existing character and talking period from the video, but the character involved in each talking period are interchanged. For example, merging descriptions like "A woman wearing a blue sweater is talking" (00:20 to 00:35) and "A man wearing a gray t-shirt is speaking" (00:05 to 00:15) to create scenarios where "A woman wearing a blue sweater is talking" (00:05 to 00:15) and "A man wearing a gray t-shirt is speaking" (00:20 to 00:30)
3. **Question Generation**: Using both correct and incorrect combination, questions related to talking are formulated.
